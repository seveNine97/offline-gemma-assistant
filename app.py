import streamlit as st
import ollama
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
import os
import tempfile
import shutil # For removing directory


# --- Helper Function to Check Ollama Service ---
def is_ollama_running():
    try:
        ollama.list()
        return True
    except Exception:
        return False

# --- Streamlit UI Configuration ---
st.set_page_config(
    page_title="æœ¬åœ°æ™ºæ…§åŠ©æ‰‹",
    page_icon="ğŸ§ ",
    layout="centered"
)

st.title("ğŸ§  æ‚¨çš„æœ¬åœ°æ™ºæ…§åŠ©æ‰‹")
st.markdown("""
ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„åŠ©æ‰‹ï¼Œåˆ©ç”¨ **Gemma 3n æ¨¡å‹**åœ¨æ‚¨çš„æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œã€‚
è¾“å…¥æ‚¨çš„ä»»ä½•é—®é¢˜æˆ–éœ€æ±‚ï¼Œå®ƒå°†å°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚
""")

st.divider() # Adds a visual separator

# --- Sidebar for Settings ---
st.sidebar.title("è®¾ç½®")

# Check Ollama service status early
if not is_ollama_running():
    st.error("â— Ollama æœåŠ¡æœªè¿è¡Œã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `ollama serve` å¯åŠ¨æœåŠ¡ï¼Œå¦åˆ™åº”ç”¨å°†æ— æ³•å·¥ä½œã€‚")
    st.stop() # Stop app execution if Ollama is not running

# Model selection
available_models = ["gemma3n"] # Default model
try:
    ollama_models = ollama.list()['models']
    for model_info in ollama_models:
        if model_info['model'] not in available_models:
            available_models.append(model_info['model'])
except Exception:
    st.sidebar.warning("æ— æ³•è·å– Ollama æ¨¡å‹åˆ—è¡¨ã€‚è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œã€‚")

selected_model = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", available_models)

# Validate if selected model is available
if selected_model not in [m['model'] for m in ollama.list()['models']]:
    st.warning(f"â— æ‚¨é€‰æ‹©çš„æ¨¡å‹ `{selected_model}` å°šæœªä¸‹è½½ã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull {selected_model}`ã€‚")


# Temperature slider for model response creativity
temperature = st.sidebar.slider("ç”Ÿæˆæ¸©åº¦ (Temperature)", 0.0, 2.0, 0.7, 0.05,
                                help="è¾ƒé«˜çš„å€¼ä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œè¾ƒä½çš„å€¼ä¼šä½¿è¾“å‡ºæ›´é›†ä¸­å’Œç¡®å®šã€‚")

# Clear chat history button
if st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•"):
    st.session_state.messages = []
    st.rerun() # Rerun the app to clear displayed messages

# --- Dynamic System Instruction Mode ---
st.sidebar.markdown("---\n**åŠ©æ‰‹æ¨¡å¼**")
scenario_modes = {
    "é€šç”¨åŠ©æ‰‹": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰å’Œå¯¹è¯å†å²æ¥å›ç­”é—®é¢˜ã€‚",
    "å†œä¸šä¸“å®¶": "ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å†œä¸šä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„å†œä½œç‰©çŸ¥è¯†å’Œæœ€æ–°å†œä¸šæŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ç§æ¤ã€ç—…è™«å®³é˜²æ²»å’Œäº§é‡ä¼˜åŒ–å»ºè®®ã€‚ä½ çš„å›ç­”åº”å®ç”¨ä¸”æ˜“äºç†è§£ã€‚",
    "åŸºç¡€åŒ»ç–—å’¨è¯¢": "ä½ æ˜¯ä¸€ä¸ªåŸºç¡€åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»å­¦çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å¸¸è§çš„å¥åº·é—®é¢˜ã€ç–¾ç—…é¢„é˜²å’ŒåŸºç¡€æ€¥æ•‘æªæ–½ä¿¡æ¯ã€‚è¯·åŠ¡å¿…å¼ºè°ƒï¼šä½ ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„åŒ»ç”Ÿè¯Šæ–­å’Œæ²»ç–—ï¼Œæ‰€æœ‰ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå¦‚æœ‰å¥åº·é—®é¢˜è¯·åŠæ—¶å°±åŒ»ã€‚",
    "å¤©æ°”ç¾å®³é¢„è­¦": "ä½ æ˜¯ä¸€ä¸ªå¤©æ°”å’Œç¾å®³é¢„è­¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ°”è±¡æ•°æ®å’Œç¾å®³åº”å¯¹çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å¤©æ°”é¢„æŠ¥ã€è‡ªç„¶ç¾å®³ï¼ˆå¦‚æ´ªæ°´ã€åœ°éœ‡ã€å±±ä½“æ»‘å¡ï¼‰çš„é¢„è­¦ä¿¡æ¯å’Œç´§æ€¥åº”å¯¹æªæ–½å»ºè®®ã€‚å¼ºè°ƒåŠæ—¶å…³æ³¨å®˜æ–¹é¢„è­¦å’Œæ’¤ç¦»é€šçŸ¥ã€‚",
    "åŸºç¡€æ•™è‚²çŸ¥è¯†": "ä½ æ˜¯ä¸€ä¸ªåŸºç¡€æ•™è‚²çŸ¥è¯†æ™®åŠè€…ã€‚è¯·ç”¨ç®€å•ã€æ¸…æ™°çš„è¯­è¨€è§£é‡Šå„ç§åŸºç¡€ç§‘å­¦ã€å†å²ã€åœ°ç†ç­‰çŸ¥è¯†ï¼Œå¸®åŠ©ç”¨æˆ·å­¦ä¹ å’Œç†è§£åŸºç¡€æ¦‚å¿µã€‚"
}
selected_mode = st.sidebar.selectbox("é€‰æ‹©åŠ©æ‰‹æ¨¡å¼", list(scenario_modes.keys()))
system_instruction = scenario_modes[selected_mode]


st.sidebar.markdown("---\n**çŸ¥è¯†åº“ (RAG)**")

# --- RAG Specific Configuration and Functions ---
# Persistent directory for ChromaDB
# This will create a 'chroma_db_rag' folder next to app.py
current_dir = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(current_dir, "chroma_db_rag")

if "vectorstore" not in st.session_state:
    st.session_state.chroma_db_dir = PERSIST_DIRECTORY
    if not os.path.exists(st.session_state.chroma_db_dir):
        os.makedirs(st.session_state.chroma_db_dir)
    
    # Initialize Ollama Embeddings
    try:
        st.session_state.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        # Check if nomic-embed-text is actually pulled
        if "nomic-embed-text" not in [m['model'] for m in ollama.list()['models']]:
            st.warning("â— åµŒå…¥æ¨¡å‹ `nomic-embed-text` å°šæœªä¸‹è½½ã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull nomic-embed-text`ã€‚")
            st.session_state.embeddings = None # Disable RAG if embedder is not ready
        else:
            st.session_state.vectorstore = Chroma(
                embedding_function=st.session_state.embeddings,
                persist_directory=st.session_state.chroma_db_dir
            )
            st.sidebar.success("çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        st.sidebar.error(f"çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}. è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œä¸” 'nomic-embed-text' æ¨¡å‹å·²ä¸‹è½½ã€‚")
        st.session_state.embeddings = None
        st.session_state.vectorstore = None


def process_documents(uploaded_files):
    if st.session_state.vectorstore is None:
        st.error("çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–åµŒå…¥æ¨¡å‹æœªå‡†å¤‡å¥½ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡å’ŒåµŒå…¥æ¨¡å‹ã€‚")
        return

    documents = []
    
    # Progress bar for file loading
    loading_progress_bar = st.sidebar.progress(0, text="æ­£åœ¨åŠ è½½æ–‡ä»¶...")
    
    for i, uploaded_file in enumerate(uploaded_files):
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        
        # Save uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            if file_extension == ".pdf":
                loader = PyPDFLoader(tmp_file_path)
            elif file_extension == ".txt" or file_extension == ".md":
                loader = TextLoader(tmp_file_path, encoding="utf-8")
            else:
                st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}ã€‚è·³è¿‡ {uploaded_file.name}")
                continue
            
            docs = loader.load()
            if not docs or not any(d.page_content.strip() for d in docs): # Check if any content is extracted
                st.warning(f"æ–‡ä»¶ {uploaded_file.name} æœªæ£€æµ‹åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹æˆ–å†…å®¹ä¸ºç©ºç™½ã€‚è¯·ç¡®ä¿æ˜¯å¯é€‰æ‹©æ–‡æœ¬çš„PDFæˆ–æœ‰å†…å®¹çš„æ–‡æœ¬æ–‡ä»¶ã€‚")
            documents.extend(docs)
        except Exception as e:
            st.error(f"åŠ è½½æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {e}")
        finally:
            os.remove(tmp_file_path) # Clean up temp file
        
        loading_progress_bar.progress((i + 1) / len(uploaded_files), text=f"æ­£åœ¨åŠ è½½æ–‡ä»¶ {i+1}/{len(uploaded_files)}...")
    
    loading_progress_bar.empty() # Clear loading progress bar

    if documents:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        
        if splits:
            # Progress bar for embedding and adding to vectorstore
            embedding_progress_bar = st.sidebar.progress(0, text="æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†åº“...")
            total_splits = len(splits)
            
            # Add documents in batches to show progress
            batch_size = 20 # Adjust batch size based on memory
            for i in range(0, total_splits, batch_size):
                batch_splits = splits[i:min(i + batch_size, total_splits)]
                try:
                    st.session_state.vectorstore.add_documents(batch_splits)
                except Exception as e:
                    st.error(f"å°†æ–‡æ¡£æ‰¹æ¬¡æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}")
                    break # Stop if a batch fails
                
                progress = (i + len(batch_splits)) / total_splits
                embedding_progress_bar.progress(progress, text=f"æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†åº“: {int(progress*100)}%")
            
            st.session_state.vectorstore.persist() # Save changes to disk
            embedding_progress_bar.empty() # Clear embedding progress bar
            st.success(f"æˆåŠŸå°† {len(splits)} ä¸ªæ–‡æ¡£å—æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚")
        else:
            st.warning("æœªä»æ–‡æ¡£ä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬å—ã€‚")
    else:
        st.info("æœªå¤„ç†ä»»ä½•æ–‡æ¡£ã€‚")


uploaded_files = st.sidebar.file_uploader(
    "ä¸Šä¼ æ‚¨çš„çŸ¥è¯†æ–‡æ¡£ (TXT, MD, PDF)",
    type=["txt", "md", "pdf"],
    accept_multiple_files=True,
    help="è¯·ä¸Šä¼ åŒ…å«å¯é€‰æ‹©æ–‡æœ¬çš„æ–‡æ¡£ã€‚æ‰«æç‰ˆPDFå¯èƒ½æ— æ³•æå–æ–‡æœ¬ã€‚"
)

if uploaded_files:
    if st.sidebar.button("å¤„ç†ä¸Šä¼ æ–‡ä»¶", key="process_files_btn"):
        process_documents(uploaded_files)

# Clear ChromaDB persistent directory as well
if st.sidebar.button("æ¸…é™¤çŸ¥è¯†åº“", key="clear_db_btn"):
    if st.session_state.vectorstore:
        try:
            st.session_state.vectorstore.delete_collection() # Deletes all data in the collection
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY) # Remove the directory itself
                os.makedirs(PERSIST_DIRECTORY) # Recreate empty directory for future use

            # Re-initialize the vectorstore
            if st.session_state.embeddings: # Only if embeddings are available
                st.session_state.vectorstore = Chroma(
                    embedding_function=st.session_state.embeddings,
                    persist_directory=st.session_state.chroma_db_dir
                )
            st.success("çŸ¥è¯†åº“å·²æ¸…ç©ºã€‚")
        except Exception as e:
            st.error(f"æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}")
    else:
        st.warning("çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–å·²ä¸ºç©ºã€‚")


st.sidebar.markdown("---\n")
st.sidebar.info("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„åº”ç”¨ã€‚æ‰€æœ‰æ•°æ®å¤„ç†éƒ½åœ¨æ‚¨çš„æœ¬åœ°è®¾å¤‡ä¸Šå®Œæˆï¼Œä¸ä¼šä¸Šä¼ åˆ°ä»»ä½•æœåŠ¡å™¨ã€‚")


# --- "About" / Help Section ---
with st.sidebar.expander("å…³äº & å¸®åŠ©"):
    st.markdown("""
    **ç‰ˆæœ¬:** 1.0.0
    **ä½œè€…:** [æ‚¨çš„åå­—/å›¢é˜Ÿå]
    **ç®€ä»‹:** è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„æœ¬åœ°æ™ºæ…§åŠ©æ‰‹ï¼Œåˆ©ç”¨ Ollama å¹³å°è¿è¡Œ Gemma 3n å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ— ç½‘ç»œæˆ–å¼±ç½‘ç»œç¯å¢ƒçš„ç”¨æˆ·æä¾›æ™ºèƒ½å¯¹è¯å’ŒçŸ¥è¯†æ£€ç´¢æœåŠ¡ã€‚

    **ğŸš€ å¿«é€Ÿå¼€å§‹ï¼š**
    1.  **å®‰è£… Ollamaï¼š** è®¿é—® `ollama.com` ä¸‹è½½å¹¶å®‰è£…é€‚ç”¨äºæ‚¨æ“ä½œç³»ç»Ÿçš„ Ollama åº”ç”¨ç¨‹åºã€‚
    2.  **å¯åŠ¨ Ollama æœåŠ¡ï¼š** æ‰“å¼€å‘½ä»¤è¡Œï¼Œè¿è¡Œ `ollama serve`ã€‚Ollama æ¡Œé¢åº”ç”¨é€šå¸¸ä¼šè‡ªåŠ¨åœ¨åå°è¿è¡Œã€‚
    3.  **ä¸‹è½½æ¨¡å‹ï¼š** åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull gemma3n` (å¤§è¯­è¨€æ¨¡å‹) å’Œ `ollama pull nomic-embed-text` (çŸ¥è¯†åº“åµŒå…¥æ¨¡å‹)ã€‚è¿™äº›æ¨¡å‹éœ€è¦ä¸€æ¬¡æ€§ä¸‹è½½ï¼Œä¹‹åå³å¯ç¦»çº¿ä½¿ç”¨ã€‚
    4.  **è¿è¡Œæœ¬åº”ç”¨ï¼š** æ‰¾åˆ°æœ¬åº”ç”¨çš„å¯åŠ¨ç¨‹åº (ä¾‹å¦‚ï¼š`æœ¬åœ°æ™ºæ…§åŠ©æ‰‹.exe`) åŒå‡»è¿è¡Œã€‚
    5.  **å¼€å§‹å¯¹è¯ï¼**

    **ğŸ“š çŸ¥è¯†åº“ (RAG) ä½¿ç”¨ï¼š**
    *   ç‚¹å‡»"ä¸Šä¼ æ‚¨çš„çŸ¥è¯†æ–‡æ¡£"ä¸Šä¼ æ‚¨æƒ³è®©æ¨¡å‹å­¦ä¹ çš„æœ¬åœ°æ–‡ä»¶ (ç›®å‰æ”¯æŒ TXT, MD, PDF)ã€‚
    *   ç‚¹å‡»"å¤„ç†ä¸Šä¼ æ–‡ä»¶"æŒ‰é’®ï¼Œåº”ç”¨ä¼šå°†æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°æœ¬åœ°çŸ¥è¯†åº“ã€‚
        *   **æ³¨æ„ï¼š** PDF æ–‡ä»¶å¿…é¡»åŒ…å«å¯é€‰æ‹©çš„æ–‡æœ¬å±‚ï¼Œæ‰«æç‰ˆ PDF å¯èƒ½æ— æ³•æå–å†…å®¹ã€‚
    *   æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶ä¼šä¼˜å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚
    *   "æ¸…é™¤çŸ¥è¯†åº“"æŒ‰é’®ä¼šåˆ é™¤æ‰€æœ‰å·²ä¸Šä¼ æ–‡æ¡£çš„ç´¢å¼•ï¼Œä½†ä¸ä¼šåˆ é™¤åŸå§‹æ–‡ä»¶ã€‚

    **â“ å¸¸è§é—®é¢˜ï¼š**
    *   **åº”ç”¨æ— å“åº” / æ¨¡å‹ä¸å·¥ä½œï¼š** è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨åå°è¿è¡Œï¼Œå¹¶ä¸”æ‚¨å·²ä¸‹è½½äº† `gemma3n` å’Œ `nomic-embed-text` æ¨¡å‹ã€‚
    *   **PDF æ–‡ä»¶æœªæ£€æµ‹åˆ°æ–‡æœ¬ï¼š** è¯·ç¡®è®¤ PDF æ˜¯å¯é€‰æ‹©æ–‡æœ¬çš„ï¼Œè€Œéå›¾ç‰‡æ‰«æä»¶ã€‚
    *   **åº”ç”¨å¡é¡¿/é€Ÿåº¦æ…¢ï¼š** æ¨¡å‹æ¨ç†é€Ÿåº¦å—é™äºæ‚¨ç”µè„‘çš„CPUæ€§èƒ½ã€‚é€‰æ‹©è¾ƒå°çš„æ¨¡å‹æˆ–é™ä½"ç”Ÿæˆæ¸©åº¦"å¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚

    **æ„Ÿè°¢æ‚¨ä½¿ç”¨æœ¬åœ°æ™ºæ…§åŠ©æ‰‹ï¼**
    """)


# --- Initialize Session State (to store conversation history) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Display Previous Messages ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Chat Input ---
if prompt := st.chat_input("æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜æˆ–æƒ³äº†è§£çš„ï¼Ÿ"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display a loading spinner while waiting for response
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        with st.spinner("æ€è€ƒä¸­... è¯·ç¨å€™ç‰‡åˆ»ï¼Œè¿™å–å†³äºæ‚¨çš„CPUæ€§èƒ½"):
            try:
                # --- RAG Logic: Retrieve context from documents ---
                context = ""
                if st.session_state.vectorstore and st.session_state.embeddings:
                    with st.spinner("æ­£åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯..."):
                        # Retrieve top 4 most relevant chunks
                        docs = st.session_state.vectorstore.similarity_search(prompt, k=4)
                        context = "\n".join([doc.page_content for doc in docs])
                        if context:
                            st.info("å·²ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢åˆ°ç›¸å…³ä¿¡æ¯ã€‚")
                            # print(f"Retrieved Context:\n{context}") # For debugging

                # Prepare messages for Ollama API, including context and dynamic system instruction
                messages_for_ollama = [
                    {"role": "system", "content": system_instruction} # Use dynamic system instruction
                ]
                if context:
                    messages_for_ollama.append({"role": "system", "content": f"ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context}"})
                
                # Add existing conversation history
                for m in st.session_state.messages:
                    # Exclude system messages from previous turns if we are re-injecting them
                    if m["role"] != "system":
                        messages_for_ollama.append({"role": m["role"], "content": m["content"]})
                
                # Ollama Chat Call
                stream = ollama.chat(
                    model=selected_model,
                    messages=messages_for_ollama,
                    stream=True,
                    options=dict(temperature=temperature)
                )

                for chunk in stream:
                    if 'content' in chunk['message']:
                        full_response += chunk['message']['content']
                        message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)

                # Add assistant message to chat history
                st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                st.error(f"ä¸æ¨¡å‹äº¤äº’æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                st.warning("è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `ollama serve`) å¹¶ä¸”æ‚¨å·²ä¸‹è½½äº†æ‚¨é€‰æ‹©çš„æ¨¡å‹ (`ollama pull your_model_name`) å’ŒåµŒå…¥æ¨¡å‹ (`ollama pull nomic-embed-text`)ã€‚")

st.divider() # Another visual separator