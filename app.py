import streamlit as st
import ollama
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
import shutil # For removing directory

# --- Helper Function to Check Ollama Service ---
def is_ollama_running():
    try:
        ollama.list()
        return True
    except Exception:
        return False

# --- Language Management ---
# Define all text strings for translation
translations = {
    "en": {
        "page_title": "Local Smart Assistant",
        "app_title": "ğŸ§  Your Local Smart Assistant",
        "app_description": "A completely offline assistant utilizing the **Gemma 3n model** running on your local device.\nInput any question or need, and it will do its best to assist you.",
        "divider": "---",
        "settings_sidebar_title": "Settings",
        "ollama_not_running_error": "â— Ollama service is not running. Please start the service by typing `ollama serve` in the command line, otherwise the application will not work.",
        "select_model": "Select Model",
        "ollama_connection_warning": "Could not connect to Ollama service or get model list. Please ensure Ollama is running.",
        "model_not_downloaded_warning": "â— Your selected model `{selected_model}` is not yet downloaded. Please run `ollama pull {selected_model}` in the command line.",
        "temperature_slider_label": "Generation Temperature (Temperature)",
        "temperature_slider_help": "Higher values will make the output more random, while lower values will make the output more focused and deterministic.",
        "clear_chat_history_button": "Clear Chat History",
        "assistant_mode_title": "Assistant Mode",
        "general_assistant_mode": "General Assistant",
        "general_assistant_instruction": "You are a helpful assistant. Please answer questions based on the provided context information (if any) and conversation history.",
        "agriculture_expert_mode": "Agriculture Expert",
        "agriculture_expert_instruction": "You are an experienced agricultural expert. Please provide professional advice on planting, pest and disease control, and yield optimization based on the provided crop knowledge and latest agricultural technologies. Your answers should be practical and easy to understand.",
        "basic_medical_consultation_mode": "Basic Medical Consultation",
        "basic_medical_consultation_instruction": "You are a basic medical information assistant. Please provide information on common health problems, disease prevention and basic first aid measures based on the provided medical knowledge. Please emphasize: you cannot replace professional medical diagnosis and treatment; all information is for reference only, and please seek medical attention promptly if you have health problems.",
        "weather_disaster_alert_mode": "Weather Disaster Alert",
        "weather_disaster_alert_instruction": "You are a weather and disaster alert assistant. Please provide weather forecasts, natural disaster (such as floods, earthquakes, landslides) warning information, and emergency response measures suggestions based on the provided meteorological data and disaster response knowledge. Emphasize timely attention to official warnings and evacuation notices.",
        "basic_education_knowledge_mode": "Basic Education Knowledge",
        "basic_education_knowledge_instruction": "You are a popularizer of basic education knowledge. Please explain various basic science, history, geography, and other knowledge in simple, clear language to help users learn and understand basic concepts.",
        "knowledge_base_rag_title": "Knowledge Base (RAG)",
        "rag_not_initialized_error": "Knowledge base not initialized or embedding model not ready, please check Ollama service and embedding model.",
        "knowledge_base_loaded_success": "Knowledge base loaded successfully!",
        "knowledge_base_init_failed": "Knowledge base initialization failed: {e}. Please ensure Ollama is running and the 'nomic-embed-text' model is downloaded.",
        "embedding_model_not_downloaded_warning": "â— Embedding model `nomic-embed-text` is not yet downloaded. Please run `ollama pull nomic-embed-text` in the command line.",
        "unsupported_file_type_warning": "Unsupported file type: {file_extension}. Skipping {file_name}",
        "no_text_extracted_warning": "File {file_name} detected no text content or content is empty. Please ensure it is a selectable text PDF or a text file with content.",
        "file_loading_failed_error": "Failed to load file {file_name}: {e}",
        "loading_files_progress": "Loading files...",
        "processing_docs_progress": "Processing documents and building knowledge base...",
        "add_docs_failed_error": "Failed to add document batch to knowledge base: {e}",
        "docs_added_success": "Successfully added {num_splits} document blocks to the knowledge base.",
        "no_valid_text_blocks_warning": "No valid text blocks extracted from documents.",
        "no_docs_processed_info": "No documents processed.",
        "upload_docs_label": "Upload Your Knowledge Documents (TXT, MD, PDF)",
        "upload_docs_help": "Please upload documents containing selectable text. Scanned PDFs may not be able to extract text.",
        "process_uploaded_files_button": "Process Uploaded Files",
        "clear_knowledge_base_button": "Clear Knowledge Base",
        "clear_knowledge_base_success": "Knowledge base cleared.",
        "clear_knowledge_base_failed": "Failed to clear knowledge base: {e}",
        "knowledge_base_empty_warning": "Knowledge base not initialized or already empty.",
        "offline_app_info": "ğŸ’¡ This is a completely offline application. All data processing is done on your local device and is not uploaded to any server.",
        "about_help_title": "About & Help",
        "version": "Version: 1.0.0",
        "author": "Author: [seveNine]",
        "introduction": "This is a completely offline local smart assistant, utilizing the Ollama platform to run Gemma 3n large language model, designed to provide intelligent conversation and knowledge retrieval services for users in environments with no or weak internet access.",
        "quick_start_title": "ğŸš€ Quick Start:",
        "install_ollama": "1. **Install Ollama:** Visit `ollama.com` to download and install the Ollama application for your operating system.",
        "start_ollama_service": "2. **Start Ollama Service:** Open the command line and run `ollama serve`. The Ollama desktop application usually runs automatically in the background.",
        "download_models": "3. **Download Models:** In the command line, run `ollama pull gemma3n` (large language model) and `ollama pull nomic-embed-text` (knowledge base embedding model). These models need to be downloaded once and can then be used offline.",
        "run_app": "4. **Run This Application:** Locate the application's executable (e.g., `æœ¬åœ°æ™ºæ…§åŠ©æ‰‹.exe`) and double-click to run it.",
        "start_chat": "5. **Start Chat!**",
        "rag_usage_title": "ğŸ“š Knowledge Base (RAG) Usage:",
        "upload_docs_rag": "  * Click \"Upload Your Knowledge Documents\" to upload local files you want the model to learn from (currently supports TXT, MD, PDF).",
        "process_files_rag": "  * Click the \"Process Uploaded Files\" button, and the application will add the document content to the local knowledge base.",
        "pdf_note_rag": "    * **Note:** PDF files must contain a selectable text layer; scanned PDFs may not be able to extract content.",
        "model_retrieval_rag": "  * The model will prioritize retrieving relevant information from the knowledge base when answering questions.",
        "clear_db_rag": "  * The \"Clear Knowledge Base\" button will delete all uploaded document indexes but will not delete the original files.",
        "faq_title": "â“ Frequently Asked Questions:",
        "app_unresponsive_faq": "  * **App Unresponsive / Model Not Working:** Please ensure the Ollama service is running in the background, and you have downloaded the `gemma3n` and `nomic-embed-text` models.",
        "pdf_text_detection_faq": "  * **PDF File Not Detecting Text:** Please confirm that the PDF is text-selectable, not a scanned image.",
        "app_lagging_faq": "  * **App Lagging/Slow:** Model inference speed is limited by your computer's CPU performance. Choosing a smaller model or lowering the \"Generation Temperature\" may help.",
        "thank_you_note": "Thank you for using the Local Smart Assistant!",
        "chat_input_placeholder": "What questions do you have or what do you want to know?",
        "thinking_spinner": "Thinking... Please wait a moment, this depends on your CPU performance",
        "model_interaction_error": "An error occurred while interacting with the model: {e}",
        "ollama_service_check_warning": "Please ensure the Ollama service is running (type `ollama serve` in the command line) and you have downloaded your selected model (`ollama pull your_model_name`).",
        "language_switch_en": "English",
        "language_switch_zh": "ä¸­æ–‡",
        "select_assistant_mode": "Select Assistant Mode",
        "select_language": "Select Language / é€‰æ‹©è¯­è¨€"
    },
    "zh": {
        "page_title": "æœ¬åœ°æ™ºæ…§åŠ©æ‰‹",
        "app_title": "ğŸ§  æ‚¨çš„æœ¬åœ°æ™ºæ…§åŠ©æ‰‹",
        "app_description": "ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„åŠ©æ‰‹ï¼Œåˆ©ç”¨ **Gemma 3n æ¨¡å‹**åœ¨æ‚¨çš„æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œã€‚\nè¾“å…¥æ‚¨çš„ä»»ä½•é—®é¢˜æˆ–éœ€æ±‚ï¼Œå®ƒå°†å°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚",
        "divider": "---",
        "settings_sidebar_title": "è®¾ç½®",
        "ollama_not_running_error": "â— Ollama æœåŠ¡æœªè¿è¡Œã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `ollama serve` å¯åŠ¨æœåŠ¡ï¼Œå¦åˆ™åº”ç”¨å°†æ— æ³•å·¥ä½œã€‚",
        "select_model": "é€‰æ‹©æ¨¡å‹",
        "ollama_connection_warning": "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡æˆ–è·å–æ¨¡å‹åˆ—è¡¨ã€‚è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œã€‚",
        "model_not_downloaded_warning": "â— æ‚¨é€‰æ‹©çš„æ¨¡å‹ `{selected_model}` å°šæœªä¸‹è½½ã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull {selected_model}`ã€‚",
        "temperature_slider_label": "ç”Ÿæˆæ¸©åº¦ (Temperature)",
        "temperature_slider_help": "è¾ƒé«˜çš„å€¼ä¼šä½¿è¾“å‡ºæ›´éšæœºï¼Œè¾ƒä½çš„å€¼ä¼šä½¿è¾“å‡ºæ›´é›†ä¸­å’Œç¡®å®šã€‚",
        "clear_chat_history_button": "æ¸…ç©ºèŠå¤©è®°å½•",
        "assistant_mode_title": "åŠ©æ‰‹æ¨¡å¼",
        "general_assistant_mode": "é€šç”¨åŠ©æ‰‹",
        "general_assistant_instruction": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰å’Œå¯¹è¯å†å²æ¥å›ç­”é—®é¢˜ã€‚",
        "agriculture_expert_mode": "å†œä¸šä¸“å®¶",
        "agriculture_expert_instruction": "ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„å†œä¸šä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„å†œä½œç‰©çŸ¥è¯†å’Œæœ€æ–°å†œä¸šæŠ€æœ¯ï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ç§æ¤ã€ç—…è™«å®³é˜²æ²»å’Œäº§é‡ä¼˜åŒ–å»ºè®®ã€‚ä½ çš„å›ç­”åº”å®ç”¨ä¸”æ˜“äºç†è§£ã€‚",
        "basic_medical_consultation_mode": "åŸºç¡€åŒ»ç–—å’¨è¯¢",
        "basic_medical_consultation_instruction": "ä½ æ˜¯ä¸€ä¸ªåŸºç¡€åŒ»ç–—ä¿¡æ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„åŒ»å­¦çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å¸¸è§çš„å¥åº·é—®é¢˜ã€ç–¾ç—…é¢„é˜²å’ŒåŸºç¡€æ€¥æ•‘æªæ–½ä¿¡æ¯ã€‚è¯·åŠ¡å¿…å¼ºè°ƒï¼šä½ ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„åŒ»ç”Ÿè¯Šæ–­å’Œæ²»ç–—ï¼Œæ‰€æœ‰ä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå¦‚æœ‰å¥åº·é—®é¢˜è¯·åŠæ—¶å°±åŒ»ã€‚",
        "weather_disaster_alert_mode": "å¤©æ°”ç¾å®³é¢„è­¦",
        "weather_disaster_alert_instruction": "ä½ æ˜¯ä¸€ä¸ªå¤©æ°”å’Œç¾å®³é¢„è­¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ°”è±¡æ•°æ®å’Œç¾å®³åº”å¯¹çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å¤©æ°”é¢„æŠ¥ã€è‡ªç„¶ç¾å®³ï¼ˆå¦‚æ´ªæ°´ã€åœ°éœ‡ã€å±±ä½“æ»‘å¡ï¼‰çš„é¢„è­¦ä¿¡æ¯å’Œç´§æ€¥åº”å¯¹æªæ–½å»ºè®®ã€‚å¼ºè°ƒåŠæ—¶å…³æ³¨å®˜æ–¹é¢„è­¦å’Œæ’¤ç¦»é€šçŸ¥ã€‚",
        "basic_education_knowledge_mode": "åŸºç¡€æ•™è‚²çŸ¥è¯†",
        "basic_education_knowledge_instruction": "ä½ æ˜¯ä¸€ä¸ªåŸºç¡€æ•™è‚²çŸ¥è¯†æ™®åŠè€…ã€‚è¯·ç”¨ç®€å•ã€æ¸…æ™°çš„è¯­è¨€è§£é‡Šå„ç§åŸºç¡€ç§‘å­¦ã€å†å²ã€åœ°ç†ç­‰çŸ¥è¯†ï¼Œå¸®åŠ©ç”¨æˆ·å­¦ä¹ å’Œç†è§£åŸºç¡€æ¦‚å¿µã€‚",
        "knowledge_base_rag_title": "çŸ¥è¯†åº“ (RAG)",
        "rag_not_initialized_error": "çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–åµŒå…¥æ¨¡å‹æœªå‡†å¤‡å¥½ï¼Œè¯·æ£€æŸ¥ Ollama æœåŠ¡å’ŒåµŒå…¥æ¨¡å‹ã€‚",
        "knowledge_base_loaded_success": "çŸ¥è¯†åº“åŠ è½½æˆåŠŸï¼",
        "knowledge_base_init_failed": "çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}. è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œä¸” 'nomic-embed-text' æ¨¡å‹å·²ä¸‹è½½ã€‚",
        "embedding_model_not_downloaded_warning": "â— åµŒå…¥æ¨¡å‹ `nomic-embed-text` å°šæœªä¸‹è½½ã€‚è¯·åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull nomic-embed-text`ã€‚",
        "unsupported_file_type_warning": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}ã€‚è·³è¿‡ {file_name}",
        "no_text_extracted_warning": "æ–‡ä»¶ {file_name} æœªæ£€æµ‹åˆ°ä»»ä½•æ–‡æœ¬å†…å®¹æˆ–å†…å®¹ä¸ºç©ºç™½ã€‚è¯·ç¡®ä¿æ˜¯å¯é€‰æ‹©æ–‡æœ¬çš„PDFæˆ–æœ‰å†…å®¹çš„æ–‡æœ¬æ–‡ä»¶ã€‚",
        "file_loading_failed_error": "åŠ è½½æ–‡ä»¶ {file_name} å¤±è´¥: {e}",
        "loading_files_progress": "æ­£åœ¨åŠ è½½æ–‡ä»¶...",
        "processing_docs_progress": "æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºçŸ¥è¯†åº“...",
        "add_docs_failed_error": "å°†æ–‡æ¡£æ‰¹æ¬¡æ·»åŠ åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}",
        "docs_added_success": "æˆåŠŸå°† {num_splits} ä¸ªæ–‡æ¡£å—æ·»åŠ åˆ°çŸ¥è¯†åº“ã€‚",
        "no_valid_text_blocks_warning": "æœªä»æ–‡æ¡£ä¸­æå–åˆ°ä»»ä½•æœ‰æ•ˆæ–‡æœ¬å—ã€‚",
        "no_docs_processed_info": "æœªå¤„ç†ä»»ä½•æ–‡æ¡£ã€‚",
        "upload_docs_label": "ä¸Šä¼ æ‚¨çš„çŸ¥è¯†æ–‡æ¡£ (TXT, MD, PDF)",
        "upload_docs_help": "è¯·ä¸Šä¼ åŒ…å«å¯é€‰æ‹©æ–‡æœ¬çš„æ–‡æ¡£ã€‚æ‰«æç‰ˆPDFå¯èƒ½æ— æ³•æå–æ–‡æœ¬ã€‚",
        "process_uploaded_files_button": "å¤„ç†ä¸Šä¼ æ–‡ä»¶",
        "clear_knowledge_base_button": "æ¸…é™¤çŸ¥è¯†åº“",
        "clear_knowledge_base_success": "çŸ¥è¯†åº“å·²æ¸…ç©ºã€‚",
        "clear_knowledge_base_failed": "æ¸…ç©ºçŸ¥è¯†åº“å¤±è´¥: {e}",
        "knowledge_base_empty_warning": "çŸ¥è¯†åº“æœªåˆå§‹åŒ–æˆ–å·²ä¸ºç©ºã€‚",
        "offline_app_info": "ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„åº”ç”¨ã€‚æ‰€æœ‰æ•°æ®å¤„ç†éƒ½åœ¨æ‚¨çš„æœ¬åœ°è®¾å¤‡ä¸Šå®Œæˆï¼Œä¸ä¼šä¸Šä¼ åˆ°ä»»ä½•æœåŠ¡å™¨ã€‚",
        "about_help_title": "å…³äº & å¸®åŠ©",
        "version": "ç‰ˆæœ¬: 1.0.0",
        "author": "ä½œè€…: [seveNine]",
        "introduction": "è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç¦»çº¿çš„æœ¬åœ°æ™ºæ…§åŠ©æ‰‹ï¼Œåˆ©ç”¨ Ollama å¹³å°è¿è¡Œ Gemma 3n å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºæ— ç½‘ç»œæˆ–å¼±ç½‘ç»œç¯å¢ƒçš„ç”¨æˆ·æä¾›æ™ºèƒ½å¯¹è¯å’ŒçŸ¥è¯†æ£€ç´¢æœåŠ¡ã€‚",
        "quick_start_title": "ğŸš€ å¿«é€Ÿå¼€å§‹ï¼š",
        "install_ollama": "1. **å®‰è£… Ollamaï¼š** è®¿é—® `ollama.com` ä¸‹è½½å¹¶å®‰è£…é€‚ç”¨äºæ‚¨æ“ä½œç³»ç»Ÿçš„ Ollama åº”ç”¨ç¨‹åºã€‚",
        "start_ollama_service": "2. **å¯åŠ¨ Ollama æœåŠ¡ï¼š** æ‰“å¼€å‘½ä»¤è¡Œï¼Œè¿è¡Œ `ollama serve`ã€‚Ollama æ¡Œé¢åº”ç”¨é€šå¸¸ä¼šè‡ªåŠ¨åœ¨åå°è¿è¡Œã€‚",
        "download_models": "3. **ä¸‹è½½æ¨¡å‹ï¼š** åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `ollama pull gemma3n` (å¤§è¯­è¨€æ¨¡å‹) å’Œ `ollama pull nomic-embed-text` (çŸ¥è¯†åº“åµŒå…¥æ¨¡å‹)ã€‚è¿™äº›æ¨¡å‹éœ€è¦ä¸€æ¬¡æ€§ä¸‹è½½ï¼Œä¹‹åå³å¯ç¦»çº¿ä½¿ç”¨ã€‚",
        "run_app": "4. **è¿è¡Œæœ¬åº”ç”¨ï¼š** æ‰¾åˆ°æœ¬åº”ç”¨çš„å¯åŠ¨ç¨‹åº (ä¾‹å¦‚ï¼š`æœ¬åœ°æ™ºæ…§åŠ©æ‰‹.exe`) åŒå‡»è¿è¡Œã€‚",
        "start_chat": "5. **å¼€å§‹å¯¹è¯ï¼**",
        "rag_usage_title": "ğŸ“š çŸ¥è¯†åº“ (RAG) ä½¿ç”¨ï¼š",
        "upload_docs_rag": "  * ç‚¹å‡»\"ä¸Šä¼ æ‚¨çš„çŸ¥è¯†æ–‡æ¡£\"ä¸Šä¼ æ‚¨æƒ³è®©æ¨¡å‹å­¦ä¹ çš„æœ¬åœ°æ–‡ä»¶ (ç›®å‰æ”¯æŒ TXT, MD, PDF)ã€‚",
        "process_files_rag": "  * ç‚¹å‡»\"å¤„ç†ä¸Šä¼ æ–‡ä»¶\"æŒ‰é’®ï¼Œåº”ç”¨ä¼šå°†æ–‡æ¡£å†…å®¹æ·»åŠ åˆ°æœ¬åœ°çŸ¥è¯†åº“ã€‚",
        "pdf_note_rag": "    * **æ³¨æ„ï¼š** PDF æ–‡ä»¶å¿…é¡»åŒ…å«å¯é€‰æ‹©çš„æ–‡æœ¬å±‚ï¼Œæ‰«æç‰ˆ PDF å¯èƒ½æ— æ³•æå–å†…å®¹ã€‚",
        "model_retrieval_rag": "  * æ¨¡å‹åœ¨å›ç­”é—®é¢˜æ—¶ä¼šä¼˜å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚",
        "clear_db_rag": "  * \"æ¸…é™¤çŸ¥è¯†åº“\"æŒ‰é’®ä¼šåˆ é™¤æ‰€æœ‰å·²ä¸Šä¼ æ–‡æ¡£çš„ç´¢å¼•ï¼Œä½†ä¸ä¼šåˆ é™¤åŸå§‹æ–‡ä»¶ã€‚",
        "faq_title": "â“ å¸¸è§é—®é¢˜ï¼š",
        "app_unresponsive_faq": "  * **åº”ç”¨æ— å“åº” / æ¨¡å‹ä¸å·¥ä½œï¼š** è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨åå°è¿è¡Œï¼Œå¹¶ä¸”æ‚¨å·²ä¸‹è½½äº† `gemma3n` å’Œ `nomic-embed-text` æ¨¡å‹ã€‚",
        "pdf_text_detection_faq": "  * **PDF æ–‡ä»¶æœªæ£€æµ‹åˆ°æ–‡æœ¬ï¼š** è¯·ç¡®è®¤ PDF æ˜¯å¯é€‰æ‹©æ–‡æœ¬çš„ï¼Œè€Œéå›¾ç‰‡æ‰«æä»¶ã€‚",
        "app_lagging_faq": "  * **åº”ç”¨å¡é¡¿/é€Ÿåº¦æ…¢ï¼š** æ¨¡å‹æ¨ç†é€Ÿåº¦å—é™äºæ‚¨ç”µè„‘çš„CPUæ€§èƒ½ã€‚é€‰æ‹©è¾ƒå°çš„æ¨¡å‹æˆ–é™ä½\"ç”Ÿæˆæ¸©åº¦\"å¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚",
        "thank_you_note": "æ„Ÿè°¢æ‚¨ä½¿ç”¨æœ¬åœ°æ™ºæ…§åŠ©æ‰‹ï¼",
        "chat_input_placeholder": "æ‚¨æœ‰ä»€ä¹ˆé—®é¢˜æˆ–æƒ³äº†è§£çš„ï¼Ÿ",
        "thinking_spinner": "æ€è€ƒä¸­... è¯·ç¨å€™ç‰‡åˆ»ï¼Œè¿™å–å†³äºæ‚¨çš„CPUæ€§èƒ½",
        "model_interaction_error": "ä¸æ¨¡å‹äº¤äº’æ—¶å‘ç”Ÿé”™è¯¯: {e}",
        "ollama_service_check_warning": "è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ (åœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ `ollama serve`) å¹¶ä¸”æ‚¨å·²ä¸‹è½½äº†æ‚¨é€‰æ‹©çš„æ¨¡å‹ (`ollama pull your_model_name`)ã€‚",
        "language_switch_en": "English",
        "language_switch_zh": "ä¸­æ–‡",
        "select_assistant_mode": "Select Assistant Mode",
        "select_language": "Select Language / é€‰æ‹©è¯­è¨€"
    }
}

if "language" not in st.session_state:
    st.session_state.language = "en" # Default language is English

def get_text(key):
    return translations[st.session_state.language].get(key, str(key))


# --- Streamlit UI Configuration ---
st.set_page_config(
    page_title=get_text("page_title"),
    page_icon="ğŸ§ ",
    layout="centered"
)

st.title(get_text("app_title"))
st.markdown(get_text("app_description"))

st.divider() # Adds a visual separator

# --- Sidebar for Settings ---
st.sidebar.title(get_text("settings_sidebar_title"))

# Language selection at the very top of sidebar
language_options = {
    get_text("language_switch_en"): "en",
    get_text("language_switch_zh"): "zh"
}
selected_display_language = st.sidebar.radio(
    get_text("select_language"),
    list(language_options.keys()),
    index=list(language_options.values()).index(st.session_state.language)
)

if language_options[selected_display_language] != st.session_state.language:
    st.session_state.language = language_options[selected_display_language]
    st.rerun()


# Check Ollama service status early
if not is_ollama_running():
    st.error(get_text("ollama_not_running_error"))
    st.stop() # Stop app execution if Ollama is not running

# Model selection
available_models = ["gemma3n:latest"] # Default model
try:
    ollama_models = ollama.list()['models']
    for model_info in ollama_models:
        if model_info['model'] not in available_models:
            available_models.append(model_info['model'])
except Exception:
    st.sidebar.warning(get_text("ollama_connection_warning"))

selected_model = st.sidebar.selectbox(get_text("select_model"), available_models)

# Validate if selected model is available
if selected_model not in [m['model'] for m in ollama.list()['models']]:
    st.warning(get_text("model_not_downloaded_warning").format(selected_model=selected_model))


# Temperature slider for model response creativity
temperature = st.sidebar.slider(get_text("temperature_slider_label"), 0.0, 1.0, 0.7, 0.05,
                                help=get_text("temperature_slider_help"))

# Clear chat history button
if st.sidebar.button(get_text("clear_chat_history_button")):
    st.session_state.messages = []
    st.rerun() # Rerun the app to clear displayed messages

# --- Dynamic System Instruction Mode ---
st.sidebar.markdown(get_text("divider") + "\n**" + get_text("assistant_mode_title") + "**")
scenario_modes = {
    get_text("general_assistant_mode"): get_text("general_assistant_instruction"),
    get_text("agriculture_expert_mode"): get_text("agriculture_expert_instruction"),
    get_text("basic_medical_consultation_mode"): get_text("basic_medical_consultation_instruction"),
    get_text("weather_disaster_alert_mode"): get_text("weather_disaster_alert_instruction"),
    get_text("basic_education_knowledge_mode"): get_text("basic_education_knowledge_instruction")
}
selected_mode = st.sidebar.selectbox(get_text("select_assistant_mode"), list(scenario_modes.keys()))
system_instruction = scenario_modes[selected_mode]


st.sidebar.markdown(get_text("divider") + "\n**" + get_text("knowledge_base_rag_title") + "**")

# --- RAG Specific Configuration and Functions ---
# Persistent directory for ChromaDB
# This will create a 'chroma_db_rag' folder next to app.py
current_dir = os.path.dirname(os.path.abspath(__file__))
PERSIST_DIRECTORY = os.path.join(current_dir, "chroma_db_rag")

if "vectorstore" not in st.session_state:
    st.session_state.chroma_db_dir = PERSIST_DIRECTORY
    if not os.path.exists(st.session_state.chroma_db_dir):
        os.makedirs(st.session_state.chroma_db_dir)
    
    # Initialize Ollama Embeddings
    try:
        st.session_state.embeddings = OllamaEmbeddings(model="nomic-embed-text")
        # Check if nomic-embed-text is actually pulled
        if "nomic-embed-text:latest" not in [m['model'] for m in ollama.list()['models']]:
            st.warning(get_text("embedding_model_not_downloaded_warning"))
            st.session_state.embeddings = None # Disable RAG if embedder is not ready
        else:
            st.session_state.vectorstore = Chroma(
                embedding_function=st.session_state.embeddings,
                persist_directory=st.session_state.chroma_db_dir
            )
            st.sidebar.success(get_text("knowledge_base_loaded_success"))
    except Exception as e:
        st.sidebar.error(get_text("knowledge_base_init_failed").format(e=e))
        st.session_state.embeddings = None
        st.session_state.vectorstore = None


def process_documents(uploaded_files):
    if st.session_state.vectorstore is None:
        st.error(get_text("rag_not_initialized_error"))
        return

    documents = []
    
    # Progress bar for file loading
    loading_progress_bar = st.sidebar.progress(0, text=get_text("loading_files_progress"))
    
    for i, uploaded_file in enumerate(uploaded_files):
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        
        # Save uploaded file to a temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name

        try:
            if file_extension == ".pdf":
                loader = PyPDFLoader(tmp_file_path)
            elif file_extension == ".txt" or file_extension == ".md":
                loader = TextLoader(tmp_file_path, encoding="utf-8")
            else:
                st.warning(get_text("unsupported_file_type_warning").format(file_extension=file_extension, file_name=uploaded_file.name))
                continue
            
            docs = loader.load()
            if not docs or not any(d.page_content.strip() for d in docs): # Check if any content is extracted
                st.warning(get_text("no_text_extracted_warning").format(file_name=uploaded_file.name))
            documents.extend(docs)
        except Exception as e:
            st.error(get_text("file_loading_failed_error").format(file_name=uploaded_file.name, e=e))
        finally:
            os.remove(tmp_file_path) # Clean up temp file
        
        loading_progress_bar.progress((i + 1) / len(uploaded_files), text=get_text("loading_files_progress") + f" {i+1}/{len(uploaded_files)}...")
    
    loading_progress_bar.empty() # Clear loading progress bar

    if documents:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        
        if splits:
            # Progress bar for embedding and adding to vectorstore
            embedding_progress_bar = st.sidebar.progress(0, text=get_text("processing_docs_progress"))
            total_splits = len(splits)
            
            # Add documents in batches to show progress
            batch_size = 20 # Adjust batch size based on memory
            for i in range(0, total_splits, batch_size):
                batch_splits = splits[i:min(i + batch_size, total_splits)]
                try:
                    st.session_state.vectorstore.add_documents(batch_splits)
                except Exception as e:
                    st.error(get_text("add_docs_failed_error").format(e=e))
                    break # Stop if a batch fails
                
                progress = (i + len(batch_splits)) / total_splits
                embedding_progress_bar.progress(progress, text=get_text("processing_docs_progress") + f": {int(progress*100)}%")
            
            st.session_state.vectorstore.persist() # Save changes to disk
            embedding_progress_bar.empty() # Clear embedding progress bar
            st.success(get_text("docs_added_success").format(num_splits=len(splits)))
        else:
            st.warning(get_text("no_valid_text_blocks_warning"))
    else:
        st.info(get_text("no_docs_processed_info"))


uploaded_files = st.sidebar.file_uploader(
    get_text("upload_docs_label"),
    type=["txt", "md", "pdf"],
    accept_multiple_files=True,
    help=get_text("upload_docs_help")
)

if uploaded_files:
    if st.sidebar.button(get_text("process_uploaded_files_button"), key="process_files_btn"):
        process_documents(uploaded_files)

# Clear ChromaDB persistent directory as well
if st.sidebar.button(get_text("clear_knowledge_base_button"), key="clear_db_btn"):
    if st.session_state.vectorstore:
        try:
            st.session_state.vectorstore.delete_collection() # Deletes all data in the collection
            if os.path.exists(PERSIST_DIRECTORY):
                shutil.rmtree(PERSIST_DIRECTORY) # Remove the directory itself
                os.makedirs(PERSIST_DIRECTORY) # Recreate empty directory for future use

            # Re-initialize the vectorstore
            if st.session_state.embeddings: # Only if embeddings are available
                st.session_state.vectorstore = Chroma(
                    embedding_function=st.session_state.embeddings,
                    persist_directory=st.session_state.chroma_db_dir
                )
            st.success(get_text("clear_knowledge_base_success"))
        except Exception as e:
            st.error(get_text("clear_knowledge_base_failed").format(e=e))
    else:
        st.warning(get_text("knowledge_base_empty_warning"))


st.sidebar.markdown(get_text("divider") + "\n")
st.sidebar.info(get_text("offline_app_info"))


# --- "About" / Help Section ---
with st.sidebar.expander(get_text("about_help_title")):
    st.markdown(f"""
    **{get_text("version")}**
    **{get_text("author")}**
    **{get_text("introduction")}**

    **{get_text("quick_start_title")}**
    1.  **{get_text("install_ollama")}**
    2.  **{get_text("start_ollama_service")}**
    3.  **{get_text("download_models")}**
    4.  **{get_text("run_app")}**
    5.  **{get_text("start_chat")}**

    **{get_text("rag_usage_title")}**
    *   {get_text("upload_docs_rag")}
    *   {get_text("process_files_rag")}
        *   **{get_text("pdf_note_rag")}**
    *   {get_text("model_retrieval_rag")}
    *   {get_text("clear_db_rag")}

    **{get_text("faq_title")}**
    *   {get_text("app_unresponsive_faq")}
    *   {get_text("pdf_text_detection_faq")}
    *   {get_text("app_lagging_faq")}

    **{get_text("thank_you_note")}**
    """)


# --- Initialize Session State (to store conversation history) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Display Previous Messages ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Chat Input ---
if prompt := st.chat_input(get_text("chat_input_placeholder")):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display a loading spinner while waiting for response
    with st.chat_message("assistant"):
        # Create a placeholder for streaming response
        message_placeholder = st.empty()
        full_response = ""
        with st.spinner(get_text("thinking_spinner")):
            try:
                # Call Ollama API to get response from selected model
                stream = ollama.chat(
                    model=selected_model, # Use the selected model from sidebar
                    messages=[
                        {"role": m["role"], "content": m["content"]}
                        for m in st.session_state.messages
                    ],
                    stream=True, # Enable streaming responses
                    options=dict(temperature=temperature) # Apply temperature setting
                )

                for chunk in stream:
                    if 'content' in chunk['message']:
                        full_response += chunk['message']['content']
                        # Update the placeholder with the current full response and a blinking cursor
                        message_placeholder.markdown(full_response + "â–Œ")
                # After streaming is complete, display the final response without the cursor
                message_placeholder.markdown(full_response)

                # Add assistant message to chat history
                st.session_state.messages.append({"role": "assistant", "content": full_response})

            except Exception as e:
                st.error(get_text("model_interaction_error").format(e=e))
                st.warning(get_text("ollama_service_check_warning"))

st.divider() # Another visual separator